<!DOCTYPE html>
<html lang="zh">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.61.0" />

    
    
    

<title>Transformer01 • YF-Blog</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformer01"/>
<meta name="twitter:description" content="Transformer结构分析01"/>

<meta property="og:title" content="Transformer01" />
<meta property="og:description" content="Transformer结构分析01" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yf-gooo.github.io/posts/transformer01/" />
<meta property="article:published_time" content="2019-12-14T15:26:44+08:00" />
<meta property="article:modified_time" content="2019-12-14T15:26:44+08:00" /><meta property="og:site_name" content="Title" />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.9181f25ed2263aeb878ec6f8a84f10c4ebb16150000fca8767308880bdde5ca0.css" integrity="sha256-kYHyXtImOuuHjsb4qE8QxOuxYVAAD8qHZzCIgL3eXKA=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://yf-gooo.github.io/">YF-Blog</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://yf-gooo.github.io/img/dog.jpeg" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
         一个缓慢成长中的数据科学犬 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">YF-Blog</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Portfolio</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	
	
	<a href="https://github.com/https://github.com/YF-Gooo" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/%3cusername%3e" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="mailto:your-email@example.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2019 htr3n
  
    <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>
  
</div>



  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Transformer01</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Dec 14, 2019
    
    
    
      
      
          in
          
          
              <a class="badge badge-category" href="/categories/deeplearning">DEEPLEARNING</a>
              •
          
              <a class="badge badge-category" href="/categories/nlp">NLP</a>
              •
          
              <a class="badge badge-category" href="/categories/pytorch">PYTORCH</a>
              
          
      
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/deeplearning">deeplearning</a>
           
      
          <a class="badge badge-tag" href="/tags/pytorch">pytorch</a>
           
      
          <a class="badge badge-tag" href="/tags/nlp">nlp</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 1 min read
</div>


  </header>
  
  
  <div class="post">
    <p>白瞟bert也有一年多了，都没有好好研究过transformer的具体结构，是时候仔细研究下了.<br>
代码保存在 : <a href="https://github.com/YF-Gooo/DeepLearning-NLP/blob/master/transformer/transformer.py">https://github.com/YF-Gooo/DeepLearning-NLP/blob/master/transformer/transformer.py</a></p>
<h4 id="heading">结构:</h4>
<p><img src="/img/transformer/transformer.png" alt="transformer"></p>
<h2 id="heading-1">流程:</h2>
<h3 id="heading-2">数据:</h3>
<pre><code>＃输入  
sentences = ['ich mochte ein bier P', 
             'S i want a beer', 
             'i want a beer E']
＃输出
enc_inputs＝tensor([[1, 2, 3, 4, 0]]) 
dec_inputs＝tensor([[5, 1, 2, 3, 4]]) 
target_batch＝tensor([[1, 2, 3, 4, 6]])
</code></pre>
<h3 id="heading-3">模型:</h3>
<h5 id="transformer">Transformer:</h5>
<pre><code>class Transformer(nn.Module):
    def __init__(self):
        super(Transformer, self).__init__()
        self.encoder = Encoder()
        self.decoder = Decoder()
        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)
    def forward(self, enc_inputs, dec_inputs):
        enc_outputs, enc_self_attns = self.encoder(enc_inputs)
        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)
        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]
        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns
</code></pre>
<h5 id="encoder">Encoder:</h5>
<p>没啥说的就是用了个位置向量和一个MultiHeadAttention
(基于self-attention)</p>
<h5 id="decoder">Decoder:</h5>
<p>Decoder层的attention就比较有意思了,
不容易理解的主要是这两句</p>
<pre><code>    dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)
    dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)
</code></pre>
<h5 id="heading-4">第一句</h5>
<p>简单解释下，乍一看是在做dec_inputs的self attention,但因为引入了dec_self_attn_mask这个mask.
其实是在合并单词含义，为一会的并行计算做准备．</p>
<pre><code>dec_outputs的第一个元素包含dec_inputs第一个元素的信息  
dec_outputs的第二个元素包含dec_inputs前两个个元素的信息  
</code></pre>
<p>这是符合逻辑的，因为我们一会是要用dec_outputs的第一个元素去预测我们第一个单词  .
我们预测第一个单词的时候，是不知道后面的单词信息的，能用的只有我们输入的Ｓ.<br>
再次强调，现在的dec_outputs的每个元素，是多个dec_inputs信息的结合体.</p>
<h5 id="heading-5">第二句</h5>
<p>这一句就不是做self attention，做的是融合的attention,融合啥？<br>
dec_outputs已经包含dec_inputs的信息了，但这不够，我们需要更多信息也就是来自enc_output的信息.<br>
当把encode的信息融合进入dec_outputs以后我们就可以放心的去预测了.</p>
<h3 id="heading-6">总结</h3>
<p>其实总体看下来transformer绝对不能说是什么特大创新．明显有coattention等更早的论文的影子．</p>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/hugo%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA02/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Hugo博客搭建02</span>
    </a>
    
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
